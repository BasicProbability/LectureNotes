\chapter{Axiomatic Probability Theory}

\section{Axioms of Probability}
In the previous chapter, we have introduced sample spaces and event spaces. We would like to be able
to express that certain events are more (or less) likely than others. 
Therefore, we are going to measure the probability of events in a mathematically precise sense. 

\begin{Definition}[Finite Measure]\label{axioms}
A \emph{finite measure} is a function $ \mu: \mathcal{S} \rightarrow \mathbb{R} : S \mapsto \mu (S) $ 
that maps elements
from a countable set of sets $ \mathcal{S} $ (formally a \href{http://en.wikipedia.org/wiki/Sigma-algebra}
{$ \sigma $-algebra}) to real numbers. Such a measure has the following properties:
\begin{enumerate}
\item $ \mu(S) \in \mathbb{R} $ for $ S \in \mathcal{S} \, ,$
\item $ \mu\left( \underset{i = 1}{\overset{\infty}{\bigcup}} S_{i} \right)
= \underset{i = 1}{\overset{\infty}{\sum}} \mu \left( S_{i} \right) $ for disjoint sets $S_1, S_2, \ldots$ \, . \label{countableAdditivty}
\end{enumerate}
\end{Definition}

Notice that we are restricting ourselves to finite measures here, i.e. the value of the measure can never
be infinite. This restriction makes sense as probabilities are finite as well. Property \ref{countableAdditivty} is 
known as \emph{countable additivity}. 

Let 
$ S = \underset{i=1}{\overset{n}{\bigcup}} S_{i} $ for some positive natural number $ n $ and disjoint 
$ S_{i} $ and $ S_{j} = 
\emptyset $ for $ j > n $. By
countable additivity, we then get
\begin{equation}
\mu(S) = \mu(\underset{i=1}{\overset{\infty}{\bigcup}} S_{i}) = \mu \left( \underset{i=1}{\overset{n}{\bigcup}} S_{i} \cup 
\underset{j=n+1}{\overset{\infty}{\bigcup}} \emptyset \right) 
= \underset{i=1}{\overset{n}{\sum}} \mu ( S_{i} )
+ \underset{j=n+1}{\overset{\infty}{\sum}} \mu ({\emptyset})
\end{equation}

Since the $ S_{i} $ are disjoint, we
must have $ \mu(S) = \underset{i=1}{\overset{n}{\sum}} \mu (S_{i}) $ and it follows that 
$ \mu(\emptyset) = 0 $. We conclude that the empty set has measure $ 0 $ for all measures. Furthermore, we also see from the above
derivation that countable additivity implies finite additivity, i.e.
$ \mu(S) = \underset{i=1}{\overset{n}{\sum}} \mu(S_{i}) $ for finite positive $ n $ (again, this only
holds if the $ S_{i} $ are disjoint).

Examples of measures are not hard to find. In fact, we have already seen a measure,
namely the function $ |\cdot| $ that counts the elements of a set (check yourself that it really is a 
measure). Another measure is the Dirac-measure that is related to the characteristic
function of a set. While the characteristic function tells you whether any object belongs to a given set,
the Dirac-measure tells you whether any set contains a given object. Let us call the object in question
$ a $. Then its Dirac measure $ \delta_{a}(S) = 1 $ iff $ a \in S $ and 0 otherwise (check yourself that the Dirac-measure indeed is a measure).

Apart from these examples, there is one measure, however, that is going to be the star of the rest of this 
script, namely the \textbf{probability measure}.

\begin{Definition}[Probability measure]\label{def:probmeasure}
A probability measure \\ $ \mathbb{P}: \mathcal{A} \rightarrow \mathbb{R}, A \mapsto \mathbb{P}(A) $
on an event space $ \mathcal{A} $ associated with a sample space $ \Omega $ has the
following properties:
\begin{enumerate}
\item $ \mathbb{P}(A) \geq 0 $ for all $ A \in \mathcal{A} \,$,
\item $ \mathbb{P}\left( \underset{i = 1}{\overset{\infty}{\bigcup}} A_{i} \right)
= \underset{i = 1}{\overset{\infty}{\sum}} \mathbb{P} \left( A_{i} \right) \,$ for disjoint events $A_1,A_2,\ldots$ \, ,  \label{union}
\item $ \mathbb{P}(\Omega) = 1 \,$. \label{unity}
\end{enumerate}
\end{Definition}

Notice that we only added Property~\ref{unity} to the general definition of a measure. Hence, a
\textbf{probability} (the value that the probability measure assigns to an event) will always lie in the real interval 
$[0,1]$. The above three axioms for a probability measure are often referred to as \emph{axioms of probability}
or \emph{Kolmogorov axioms} after their inventor \href{https://en.wikipedia.org/wiki/Andrey_Kolmogorov}{Andrey
Kolmogorov}.

We have already discussed uniform probabilities in the previous chapter. We can now formally explain
what we meant by that. The uniform probability measure $ \mathbb{P} $ has the property that
$ \mathbb{P}(\{\omega\}) = \frac{1}{|\Omega|} $ for all $ \omega \in \Omega $. At this point, the
distinction between sample and event spaces becomes important. We cannot measure the elements of a
sample space, only the elements of an event space! Recall our convention that we will always assume
that $ \mathcal{A} = \mathcal{P}(\Omega) $ which obviously contains a singleton for each element in
$ \Omega $. Using this assumption, the uniform probability measure is indeed well-defined. Whenever we talk about
\textit{uniform probability}, we either mean the uniform probability measure or, more often, the real
value $ \frac{1}{|\Omega|} $ to which this measure uniformly evaluates.

In order to create a tight relationship between a sample space, an event space and a probability measure,
we introduce the concept of a \textbf{probability space}. Probability spaces are also known as 
\textbf{(probabilistic) experiments}.

\begin{Definition}[Probability space] \label{def:ProbabilitySpace}
A probability space is a triple $ (\Omega, \mathcal{A}, \mathbb{P}) $, consisting of a sample space $ \Omega $,
an event space $ \mathcal{A} $ and a probability measure $ \mathbb{P} $.
\end{Definition}

If we roll a die, for example, we have the sample space $ \Omega = \{1,2,3,4,5,6\} $ and, by 
convention, the event space $ \mathcal{A} = \mathcal{P}(\Omega) $. If we add the uniform probability measure, 
we have constructed  a \emph{probabilistic experiment}. We can use it to answer a couple of questions. For example, we 
might wonder about the probability of obtaining an even number. By Property~\ref{union} of our definition, this 
probability is given by
\begin{align}
\mathbb{P}(\{2,4,6)\}) &= \mathbb{P}(\{2\} \cup \{4\} \cup \{6\}) \\
&= \mathbb{P}(\{2\}) + \mathbb{P}(\{4\})
+ \mathbb{P}(\{6\}) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}
\end{align}

Notice that this calculation is rather cumbersome. After all, we might just have evaluated 
$ \mathbb{P}(\{2,4,6\}) $ directly. This is because by convention we have $ \mathcal{A} = \mathcal{P}(\Omega) $ which certainly contains $ \{2,4,6\} $.
Since the probability measure is defined on $ \mathcal{A} $, it must map $ \{2,4,6\} $ to some real number.
However, the above calculation points to an interesting fact. In order
to fully specify a probability measure, is suffices to specify the measure on the singleton sets of the
event space. By countable additivity, this assignment already specifies the measure on the entire event space, as we can
construct any event as a countable union of singletons.

It is important to point out that we just chose the uniform probability measure as the one that seems ``natural'' for
a die roll. However, nobody is forcing us to do so. In fact, Definition~\ref{def:ProbabilitySpace} allows us to impose arbitrary probability measures.

\begin{Exercise}
Let us consider a rigged die. Take $ (\Omega, \mathcal{A}, \mathbb{P}) $ with $ \Omega $ and $ \mathcal{A} = \mathcal{P}(\Omega) $ 
as in the uniform die-roll example before, but use the 
probability measure specified by \\ $ \mathbb{P} = \{(\{1\},0), (\{2\}, \frac{1}{12}), (\{3\}, \frac{1}{6}), (\{4\}, \frac{1}{6}), (\{5\}, \frac{1}{3}),
(\{6\},\frac{1}{4}) \} $.
\begin{enumerate}
\item Verify that $ \mathbb{P} $ is indeed a probability measure.
\item Compute the probability of obtaining a number strictly smaller than $ 5 $ in this experiment.
\end{enumerate}
\end{Exercise}

\section{Probability of Arbitrary Unions of Events}
\begin{figure}
\center
\begin{subfigure}{.4\textwidth}
\begin{venndiagram2sets}[labelA=$ E_{1} $, labelB= $ E_{2} $, labelAB= $ E_{3} $, shade=red!40]
\fillACapB
\end{venndiagram2sets}
\caption{}
\label{Venn2}
\end{subfigure}
~
\begin{subfigure}{.4\textwidth}
\begin{venndiagram3sets}[labelA=$ E_{1} $, labelB=$ E_{2} $, labelC=$ E_{3} $, labelOnlyAB=$ - $, 
labelOnlyBC=$ - $, labelOnlyAC=$ - $, labelABC=$ + $, shade=red!40]
\fillACapB
\fillACapC
\fillBCapC
\end{venndiagram3sets}
\caption{}
\label{Venn3}
\end{subfigure}
\caption{\ref{Venn2}: Two overlapping events $ E_{1} $ and $ E_{2} $. Their intersection 
(the coloured region) gets counted twice if we add up their probabilities. \\
\ref{Venn3}: Venn diagram with 3 events. First we deduct 
$ E_{1} \cap E_{2}, E_{1} \cap E_{3}, E_{2} \cap E_{3} $ in order to prevent double counting and then
we add in $ E_{1} \cap E_{2} \cap E_{3} $. Deductions and additions are indicated by pluses and minuses.}
\end{figure}

We have seen how to compute probabilities of events if they can be formed as unions of \textit{disjoint}
events. The natural question to ask is what to do if we want to compute the probability of the \emph{union
of non-disjoint events}. In order to reason about this problem, we first take a step back and think about the
outcomes of our probability space. We know that each event with non-zero probability contains at least one outcome (since
$ \mathbb{P}(\emptyset) = 0 $, we can safely ignore the empty event). Let us assume that we take the union of events
$ E_{1} $ and $ E_{2} $ with $ E_{1} \cap E_{2} = E_{3} \not = \emptyset $. This means that the outcomes
in $ E_{3} $ are contained in both $ E_{1} $ and $ E_{2} $. This situation is illustrated in Figure~\ref{Venn2}. 
If we apply the rule that we have for disjoint events, we get 
\begin{align} \label{InExFalse}
\mathbb{P}(E_{1} \cup E_{2})
=& \mathbb{P}(((E_{1}\backslash E_{3}) \cup E_{3}) \cup ((E_{2}\backslash E_{3}) \cup E_{3})) \\
=& \mathbb{P}(E_{1}\backslash E_{3}) + \mathbb{P}(E_{3}) + \mathbb{P}(E_{2}\backslash E_{3}) \nonumber
+ \mathbb{P}(E_{3})
\end{align}

\begin{Exercise}
Try to justify each of the above equalities using your knowledge about sets, event spaces and the 
probability measure. In doing so, you will realize that one of the above equalities does not hold.
Which one? 
\end{Exercise}

What happens here is that we count $ E_{3} $, the outcomes that are in both events of interest, twice.
Thus, we will need to subtract $ E_{3} $ one time. This leads us to the following formulation:
\begin{equation}
\mathbb{P}(E_{1} \cup E_{2}) = \mathbb{P}(E_{1}) + \mathbb{P}(E_{2}) - \mathbb{P}(E_{1} \cap E_{2})
\end{equation}

Notice that this is fully general in that it is true even if $ E_{1} $ and $ E_{2} $ were disjoint. In that
case, their intersection would be empty. We can generalize this principle to the (countable) union of
an arbitrary number of events. This will give us a principled way of calculating the probability of any
union of events. This calculation technique is know as the \textbf{Inclusion-Exclusion principle}.

\begin{Theorem}[Inclusion-Exclusion principle]
The probability of any (countable) union of events $ E_{1}, \ldots, E_{n} $ can be computed as
\begin{equation}\label{eq:incexc}
\mathbb{P} \left( \underset{i=1}{\overset{n}{\bigcup}} E_{i} \right) 
= \underset{i=1}{\overset{n}{\sum}} (-1)^{i+1} \left( \underset{j_{1}<\ldots<j_{i}}{\sum} 
\mathbb{P} \left(E_{j_{1}} \cap \ldots \cap E_{j_{i}} \right) \right)
\end{equation}
\end{Theorem}

We are going to proceed with a combinatorial proof of the Inclusion-Exclusion principle. It is
very elegant but invokes the \href{https://en.wikipedia.org/wiki/Binomial_theorem}{binomial theorem}.
For completeness sake we will prove the binomial theorem
at the end of this chapter. For now, just trust us that it exists and is correct.

\paragraph{Proof} We are going to focus on a particular outcome $ \omega $ that is contained in $ m $ events 
which we call without loss of generality $ E_{1}, \ldots, E_{m} $ for some $ m < n $. 
Notice that we can safely neglect all events which do not contain
$ \omega $, since $ \omega $ is not going to contribute to their probability. 

For all the $ E_{i} $, $ 1 \leq i \leq m $ in which $ \omega $ is contained, it is certainly true
that $ \omega $ is also contained in their intersections. The Inclusion-Exclusion-principle
adds up or subtracts the probabilities of intersections of a given size. Notice that any intersection of more
than $ m $ events will not contain $ \omega $ as we intersect with at least one event that does not
contain $ \omega $. Thus, we only need to consider intersections of our $ m $ $ \omega $-containing
sets.

When $ i = 1 $ intersection is trivial, as it just consists of one event. How many ways are there to pick
one out of $ m $ events? The answer is $ \binom{m}{1} $. This is the number of times that $ \omega $
contributes to the overall probability. At this point we have an overestimate of that probability 
(compare this to Equation \eqref{InExFalse}). Next we subtract the probabilities of the mutual intersections ($ i = 2 $).
By the same reasoning as before, the contribution of $ \omega $ is deducted $ \binom{m}{2} $ times
which gives us an underestimate since $ \binom{m}{1} \leq \binom{m}{2} $ for $ m \geq 3 $. Since we
are adding and subtracting in alternation, we will now keep flip-flopping between under- and over
estimates. After considering all intersections of up to $ m $ sets, we should get the correct result,
however.

What we want to prove is that the right-hand side of \eqref{eq:incexc} counts $ \omega $'s contribution 
to the overall probability exactly once (because this is what happens on the left hand-side of \eqref{eq:incexc}.
That is, we have to prove that
\begin{equation} \label{InExProofStep1}
1 = \underset{i=1}{\overset{m}{\sum}}(-1)^{i-1}\binom{m}{i}
\end{equation}

We are right on our way towards exploiting the binomial theorem. Let us first state it.
\begin{equation} \label{binomTheorem}
(p + q)^{m} = \underset{i=0}{\overset{m}{\sum}} \binom{m}{i} p^{i}q^{n-i} 
\end{equation}

Setting $p=(-1)$ and $q=1$, and multiplying both sides with $(-1)$, we obtain
\[
-(-1+1)^m = - \sum_{i=0}^n \binom{m}{i} (-1)^{i}
\]
which can be rewritten as
\begin{equation} \label{eq:rewrite}
0 = -1 + \sum_{i=1}^n \binom{m}{i} (-1)^{i+1} \, ,
\end{equation}
because $\binom{m}{0} = 1$. Equation~\eqref{eq:rewrite} implies \eqref{InExProofStep1} which we needed to prove. $ \square $\bigskip
%
%It may not be entirely obvious how to use the theorem in our situation, but hold on. First observe
%that the sum in Equation~\eqref{binomTheorem} actually starts counting at 0 whereas in Equation~\eqref{InExProofStep1}1}
%it starts counting from 1. However, we already know that $ \binom{m}{0} = 1 $. Thus we can add this 
%binomial coefficient to the left hand side in Equation~\eqref{InExProofStep1} and at the same time subtract 1 from the
%right hand side. This gives us \eqref{InExProofStep2}, which is the equality that we are going to prove
%in what follows. Since we derived it from Equation~\ref{InExProofStep1}, it implies that if \eqref{InExProofStep2}
%holds, so does \eqref{InExProofStep1}.
%\begin{equation} \label{InExProofStep2}
%\underset{i=0}{\overset{m}{\sum}}(-1)^{i}\binom{m}{i} = 0
%\end{equation} 
%
%We notice that $ 0 $ in turn can be expressed as $ (1-1) $. Furthermore, any power of 0 is still 0.
%Thus we rewrite again to get
%\begin{equation}
%\underset{i=0}{\overset{m}{\sum}}\binom{m}{i}(-1)^{i} = (1-1)^{m}
%\end{equation}
%
%This looks a lot like the binomial theorem, doesn't it? Just one component is missing. We have
%our $ p $ from Equation~\eqref{binomTheorem} which is $ -1 $ in this case. We still need to account for 
%$ q $, though. Luckily, setting $ q=1 $ and taking powers does not change anything. Thus we can safely write
%\begin{equation}
%\underset{i=0}{\overset{m}{\sum}}\binom{m}{i}(-1)^{i}1^{m-i} = (1-1)^{m}
%\end{equation} 
%
%This equality is true by the binomial theorem (Equation~\eqref{binomTheorem}) which it is an instantiation of.
%We have thus succeeded in proofing the Inclusion-Exclusion principle. If the proof is not entirely
%clear to you, please go over it again. $ \square $

At this point we have done our fair share of math and found out how to calculate the probability of a union
of events. We should ask ourselves what the probability of a union of events even tells us. Observe that an event 
occurs whenever we draw an outcome from our sample space that is contained in that event. By taking the union
of events $ E_{1}, \ldots, E_{n} $ we form a new event $ E $ that (possibly) contains more outcomes than each 
of the original events. Thus, the probability of the $ E $ will be higher than (or the same as) the 
probability of each of $ E_{1}, \ldots, E_{n} $. What we are measuring then, is the probability that 
\textit{any} of the events $ E_{1}, \ldots, E_{n} $ occur. Crucially, we do not care anymore which one of them
occurs.

What we are missing is a way to express the probability that a given number of events occur 
\textit{together}. This concept is so important that we have a dedicated name for it, that of 
\textbf{joint probability}.

\begin{Definition}[Joint probability]
The joint probability of a (countable) set of events $ \{E_{1}, \ldots, E_{n}\} $ is defined as
$$ \mathbb{P}(E_{1} \cap \ldots \cap E_{n}) $$
\end{Definition}

Wow, that was simple! We don not event need to prove another rule for calculating the joint probability.
After all, we already know how to take the intersection of sets. Annoyingly, one problem remains: our
definition of event spaces does not guarantee that they contain the intersections of their members. Or does 
it? Well, let us see whether we can ``paraphrase'' what an intersection is.

\begin{align}
E_{1} \cap E_{2} = \Omega \backslash ((\Omega \backslash E_{1}) \cup (\Omega \backslash E_{2}))
\end{align}

All the operations on the right hand side are defined for events spaces. We have thus solved our problem since
we have shown that we can indeed do intersection in event spaces.
To convince yourself that this is correct, you may want to consult Figure~\ref{Venn2}. Alternatively, you
may also just realise that this is an instance of DeMorgan's laws which you should know from set theory.
Notice that we do not claim that this is the only valid ``paraphrase''. Feel free to find others, if you like!



\section{Probability of Complements of Events}
At this point we are capable to do most probabilistic computations that we will encounter in this course.
From here on, it is all about making our lives easier. For example, how would you solve the following 
problem.

\begin{Exercise}
You are observing a panel of 200 light bulbs and you know that at least one of them will light up once you 
press a button. What is the probability that any except the 87th bulb will light up? Note: this is a 
conceptual exercise. For the very keen ones, you can obtain the probability for each bulb to be turned on
by typing the following into the Python interpreter:

\begin{lstlisting}
import numpy

probabilities = numpy.random.rand(1,200)
print probabilities/probabilities.sum()
\end{lstlisting} 
\end{Exercise}

The point of the above exercise is that it will be awfully cumbersome to compute the probability of the
union of the singletons $ {E_{i}} $ where $ 1 \leq i \leq 200 $ and $ i \not = 87 $. On the other hand
we can easily look up $ \mathbb{P}(E_{87}) $. The question is whether we can exploit this simpler calculation
to help us answer the original question. Here we will again make use of the properties of event spaces.
For any event $ E $ in our event space we also have $ \Omega \backslash E $ in the same space. Furthermore,
$ E $ and $ \Omega \backslash E $ are disjoint which by our probability axioms means that we can simply add
up their probabilities if we want to calculate the probability of their union. But what's the union
of $ E $ and $ \Omega \backslash E $? It's exactly $ \Omega $. From axiom \ref{unity} we know that 
$ \mathbb{P}(\Omega) = 1 $. By simple algebraic manipulations we find that 
\begin{equation}
\mathbb{P}(\Omega \backslash E) = 1 - \mathbb{P}(E)
\end{equation}

Thus if we want to find the probability that any but the $ 87th $ bulb will light up, we simply compute
the probability that the $ 87th $ bulb will light up will light up and subtract that from 1. This is a rather 
general strategy to simplify calculations whenever the probability of an event is hard to compute. 
Maybe the probability of the complement of that event will be easier to compute.

\begin{Exercise}
Show that in general $$ \mathbb{P}(E_{1}\backslash (E_{1}\cap E_{2})) 
= \mathbb{P}(E_{1}) - \mathbb{P}(E_{1}\cap E_{2}) $$
\end{Exercise}

\section{Conditional Probability and Independence}
After we have seen how to measure the probability of events, we are going to introduce another
tremendously important concept, that of \textbf{conditional probability} measures.

\begin{Definition}[Conditional probability measure]
The probability of an event $ E_{i} $ conditioned on another event $ E_{j} $ with $ \mathbb{P}(E_{j}) > 0 $ 
is defined as $$ \mathbb{P}(E_{i}|E_{j}) := \dfrac{\mathbb{P}(E_{i} \cap E_{j})}{\mathbb{P}(E_{j})} $$
\label{condProb}
\end{Definition}

Before we get into the math of conditional probabilities, let us try to understand the meaning of this concept. 
When we are computing the conditional probability of an event $ E_{i} $, we re-scale with the 
probability of the conditioning event $ E_{j} $. If $ E_{j} \not = \Omega $, $ \mathbb{P}(E_{j}) $
might be smaller than 1. Thus, this rescaling assumes \textit{that $ E_{j} $ has already occurred}. In other
words, we are excluding all outcomes that are not in $ E_{j} $ from further consideration (even though they
may be in $ E_{i} $). The interpretation of conditional probabilities is that they are the probabilities
of events assuming that another event has already occurred.

Another interpretation is that when working with a conditional probability measure, we are in fact working
in a new probability space, where $ \Omega_{new} = E_{2} $, i.e.\ our new sample space is the conditioning event.
Notice that this also means that our probability measure will change and become the
measure from Definition~\ref{condProb}.

Here comes the cool part: although we have introduced a new concept, all the properties of probability
measures that we know by now will seamlessly carry over to conditional probabilities, if we can prove
that the conditional probability measure is a probability measure according to our axioms.

\begin{Exercise}
Use the axioms from Definition~\ref{def:probmeasure} to prove that $ \mathbb{P}(\cdot|E_{j}) $ is a probability measure.
\end{Exercise} 

We will make use of conditional probabilities quite a lot in this course. We will later see a way in which
they help us to decompose joint probability distributions. For now, we are going to focus on the fact that
they are also related to the idea of independence of events.

\begin{Definition}[Independence]
Two events $ E_{1}, E_{2} $ are said to be independent if 
$$ \mathbb{P}(E_{1} \cap E_{2}) = \mathbb{P}(E_{1}) \times \mathbb{P}(E_{2}) $$
Independence of two events is denoted as $ E_{1} \bot E_{2} $.
\end{Definition}

This definition relates to conditional probabilities in the following way: assume that $ E_{1} \bot E_{2} $.
Then we get
\begin{equation}
\mathbb{P}(E_{1}|E_{2}) = \dfrac{\mathbb{P}(E_{1} \cap E_{2})}{\mathbb{P}(E_{2})}
= \dfrac{\mathbb{P}(E_{1}) \times
  \mathbb{P}(E_{2})}{\mathbb{P}(E_{2})} = \mathbb{P}(E_{1}) \, .
\end{equation}
Hence, independence of two events $ E_{1} \bot E_{2}$ is equivalent with $\mathbb{P}(E_{1}|E_{2}) = \mathbb{P}(E_{1}) $.

\begin{Exercise}
 Prove that $E_1 \bot E_2$ is also equivalent with $\mathbb{P}(E_{2}|E_{1}) = \mathbb{P}(E_{2}) $. 
\end{Exercise}

Independence will prove to be a useful concept in later chapters. More precisely, we will often
just \textit{assume} that two events (or random variables -- see the next chapter) are independent. Although
such an independence assumption might not always hold in practice, it will allow us to formulate much simpler probabilistic models.


\section{A Remark on the Interpretation \\ of Probabilities$^{*}$}

This concludes our introduction of axiomatic probability theory. We know that a probability is
a real number in $ [0,1] $. For all that we are going to do in this course (and in most follow-up courses)
this is fully sufficient. However, some of you may wonder what a ``natural'' interpretation of probabilities
would be. There are two dominating views on that. One postulates that if we were to take A LOT (read: almost
infinitely many) samples from a sample space, the probability of an event is its frequency amongst these
samples divided by the total number of samples taken. For those of you who know limits, this principle can be
formalized as $ \mathbb{P}(E) = \underset{n \rightarrow \infty}{lim} \dfrac{\#E}{n} $. This view
is known as the \emph{frequentist view}.

The second view postulates that probabilities are an expression for degrees of belief. Basically, 
if you assign $ \mathbb{P}(E) $ to an event $ E $, then $ \mathbb{P}(E) $ is the strength of your personal belief that
$ E $ will occur. This latter view is known as the \emph{Bayesian view}.

Which conception of probability you choose is a philosophical matter and does not really impact the math.
That is why we will not care about this issue in this course. However, it is useful to at least be aware
of these two views (if only to appear knowledgeable in a conversation you may have with your philosopher 
friends).


\section{The Binomial Theorem}
The binomial theorem from Equation~\ref{binomTheorem} is actually not that hard to prove. We will do so by
induction. As a base case we choose $ m = 0 $. Then the equality is easy to see.
\begin{equation}
(p + q)^{0} = 1 = \binom{0}{0}p^{0}q^{0}
\end{equation}

Next, we assume that the theorem holds for $ m = n $. What we want to show is that it also holds for
$ m = n + 1 $. We achieve this by algebraic manipulation.

\begin{align}
(p + q)^{n+1} &= (p + q)^{n} \times (p + q) \\
&= (p+q)^{n}p + (p+q)^{n}q \\
&= p\underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n-i} + q\underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n-i} \label{indcutiveHyp} \\
&= \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i+1}q^{n-i} + \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n+1-i} \\
&= \underset{j=1}{\overset{n+1}{\sum}} \binom{n}{j-1} p^{j}q^{n+1-j} + \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n+1-i} \label{variableSwitch} \\
&= \binom{n}{n} p^{n+1}q^{(n+1)-(n+1)} + \underset{k=1}{\overset{n}{\sum}} \binom{n}{k-1} p^{k}q^{n+1-k} \label{pullOut} \\
&+ \binom{n}{0} p^{0}q^{n+1} + \underset{k=1}{\overset{n}{\sum}} \binom{n}{k} p^{k}q^{n+1-k} \nonumber \label{collapseSums}
\end{align}
\begin{align}
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\binom{n}{k} + \binom{n}{k-1}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!}{k!(n-k)!} + \dfrac{n!}{(k-1)!(n-k+1)!}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!(n+1-k)}{k!(n+1-k)!} + \dfrac{n!k}{k!(n-k+1)!}\right) p^{k}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!(n+1)}{k!(n+1-k)!}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \binom{n+1}{k} p^{k}q^{n+1-k} \\
&= \underset{i=0}{\overset{n+1}{\sum}} \binom{n}{k} p^{k}q^{n-k}
\end{align}

Let us clarify some parts of the proof. We use the induction hypothesis to expand the terms in Line~\ref{indcutiveHyp}. 
In Line~\ref{variableSwitch}, we switch the variable $ i $ in the first summand to $ j = i+1 $. The
reason why we do this is because we want to achieve congruence with the exponents of the second summand. In the following line we 
uniformly name the variables $ k $. Since $ k $ has to run over a common range, we chop off the ends of both sums that stick out. In the first
sum of line \ref{variableSwitch} that is the summand that corresponds to $ j=n+1 $ and in the second sum it is the summand that corresponds
to $ i = 0 $. We pull out both of them in line \ref{pullOut} and then collapse the sums in line \ref{collapseSums}. The following lines 
are basically just an exercise in manipulation fractions. The jump from the second-to-last to the last line is allowed because
$$ q^{n+1} = \binom{n+1}{0}p^{0}q^{n+1-0} $$ and $$ p^{n+1} = \binom{n+1}{n+1}p^{n+1}q^{(n+1)-(n+1)} $$
which are exactly the quantities that we need to add to make our sum reach from $ 0 $ to $ n+1 $. This completes the proof.

\section*{Further Reading}
A very quick and dirty introduction to measure theory is provided by Maya Gupta and can be found 
\href{https://www.ee.washington.edu/techsite/papers/documents/UWEETR-2006-0008.pdf}{here}. If you are
looking for something more extensive that also motivates event spaces and the like you may want to 
take a look at \href{http://www.stat.ncsu.edu/people/fuentes/courses/st778/lectures/ross}{this script}
by Ross Leadbatter and Stamatis Cambanis (which has also been
published as a book).




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "chapter2"
%%% End:
