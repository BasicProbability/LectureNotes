\documentclass[a4paper,11pt,leqno]{report}

\usepackage{amsmath, amssymb, mdframed, caption, subcaption}
% for Python code
\usepackage[procnames]{listings} 
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}
%
\usepackage{venndiagram}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, breaklinks=true}

\newmdtheoremenv{Definition}{Definition}[chapter]
\newmdtheoremenv{Exercise}{Exercise}[chapter]
\newmdtheoremenv{Theorem}{Theorem}[chapter]

\title{Basic Probability}
\date{}

\begin{document}

\setcounter{chapter}{1}
\chapter{Axiomatic Probability Theory}

\section{Axioms of probability}
In the previous chapter, we have introduced sample spaces and event spaces. Now we would like to be able
to express that certain events are more (or less) likely than others. 
To achieve this, we are now going to measure the probability of events in a mathematically precise sense. 

\begin{Definition}\label{axioms}
A finite measure is a function $ \mu: \mathcal{S} \rightarrow \mathbb{R} : S \mapsto \mu (S) $ 
that maps elements
from a set of sets $ \mathcal{S} $ (formally a \href{http://en.wikipedia.org/wiki/Sigma-algebra}
{$ \sigma $-algebra}) to real numbers. Such a measure has the following properties:
\begin{enumerate}
\item $ \mu(S) \in \mathbb{R} $ for $ S \in \mathcal{S} $
\item $ \mu\left( \underset{i = 1}{\overset{\infty}{\bigcup}} S_{i} \right)
= \underset{i = 1}{\overset{\infty}{\sum}} \mu \left( S_{i} \right) $ if $ S_{i}\cup S_{j} = \emptyset $ 
for $ i \not = j $ \label{countableAdditivty}
\end{enumerate}
\end{Definition}

Notice that we are restricting ourselves to finite measures here, i.e. the value of the measure can never
be infinite. This makes sense as probabilities are finite, as well. Property \ref{countableAdditivty} is 
known as  \textit{countable additivity}. 

If we let 
$ S = \underset{i=1}{\overset{n}{\bigcup}} S_{i} $ for some positive natural number $ n $ and disjoint 
$ S_{i} $ and $ S_{j} = 
\emptyset $ for $ j > n $ we get $ \mu(S) = \mu(\underset{i=1}{\overset{\infty}{\bigcup}} S_{i}) $. By
countable additivity this expands to
\begin{equation}
\mu(S) = \mu \left( \underset{i=1}{\overset{n}{\bigcup}} S_{i} \cup 
\underset{j=n+1}{\overset{\infty}{\bigcup}} \emptyset \right) 
= \underset{i=1}{\overset{n}{\sum}} \mu ( S_{i} )
+ \underset{j=n+1}{\overset{\infty}{\sum}} \mu ({\emptyset})
\end{equation}

Since the $ S_{i} $ are disjoint, we
must have $ \mu(S) = \underset{i=1}{\overset{n}{\sum}} \mu (S_{i}) $ and it follows that 
$ \mu(\emptyset) = 0 $.
That is the empty set has measure $ 0 $ holds true for all measures. Furthermore, we also see from the above
derivation that countable additivity implies finite additivity, i.e.
$ \mu(S) = \underset{i=1}{\overset{n}{\sum}} \mu(S_{i}) $ for finite positive $ n $ (again, this only
holds if the $ S_{i} $ are disjoint).

Examples of measures are not hard to find. In fact, we have already seen a measure,
namely the function $ |\cdot| $ that counts the elements of a set (check yourself that it really is a 
measure). Another measure is the Dirac-measure that is related to the characteristic
function of a set. While the characteristic function tells you whether any object belongs to a given set,
the Dirac-measure tells you whether any set contains a given object. Let us call the object in question
$ a $. Then its Dirac measure $ \delta_{a}(S) = 1 $ iff $ a \in S $ and 0 otherwise. 

Apart from these examples there is one measure, however, that is going to be the star of the rest of this 
script, namely the \textbf{probability measure}.

\begin{Definition}
A probability measure $ \mathbb{P}: \mathcal{A} \rightarrow \mathbb{R} : A \mapsto \mathbb{P}(A) $
on an event space $ \mathcal{A} $ associated with a sample space $ \Omega $ has the
following properties.
\begin{enumerate}
\item $ \mathbb{P}(A) \geq 0 $ for all $ A \in \mathcal{A} $
\item $ \mathbb{P}\left( \underset{i = 1}{\overset{\infty}{\bigcup}} A_{i} \right)
= \underset{i = 1}{\overset{\infty}{\sum}} \mathbb{P} \left( A_{i} \right) $ \label{union}
\item $ \mathbb{P}(\Omega) = 1 $ \label{unity}
\end{enumerate}
\end{Definition}

Notice that we only added item \ref{unity} to the general definition of a measure. This means that a
\textbf{probability}, i.e. the value that the probability measure assigns to an event, will always lie in
$ [0,1] $. The above three axioms for a probability measure are often referred to as axioms of probability
or Kolmogorov axioms after their inventor \href{https://en.wikipedia.org/wiki/Andrey_Kolmogorov}{Andrey
Kolmogorov}.

We have already discussed uniform probabilities in the previous chapter. Now we can formally explain
what we meant by that. The uniform probability measure is the measure $ \mathbb{P} $ such that
$ \mathbb{P}(\{\omega\}) = \frac{1}{|\Omega|} $ for all $ \omega \in \Omega $. This is where the
distinction between sample and event spaces becomes important. We cannot measure the elements of a
sample space, only the elements of an event space! Recall our convention that we will always assume
that $ \mathcal{A} = \mathcal{P}(\Omega) $ which obviously contains a singleton for each element in
$ \Omega $. This implies the uniform probability measure is indeed well-defined. Whenever we talk about
\textit{uniform probability} we either mean the uniform probability measure or, more often, the real
value $ \frac{1}{|\Omega|} $ to which this measure uniformly evaluates.

In order to create a tight relationship between a sample space, an event space and a probability measure,
we can introduce the concept of a \textbf{probability space}. Probability spaces are also know as 
(probabilistic) \textbf{experiments}.

\begin{Definition} \label{ProbabiltySpace}
A probability space is a triple $ (\Omega, \mathcal{A}, \mathbb{P}) $, consisting of a sample space $ \Omega $,
an event space $ \mathcal{A} $ and a probability measure $ \mathbb{P} $.
\end{Definition}

If we want to roll a die, for example, we have the sample space $ \Omega = \{1,2,3,4,5,6\} $ and, by 
convention, the event space $ \mathcal{A} = \mathcal{P}(\Omega) $. If we add the uniform probability measure, 
we have constructed ourselves an experiment. We can use it to answer a couple of questions. For example, we 
might ask what is the probability of obtaining an even number. By item \ref{union} of our definition, this 
probability is given as
\begin{equation}
\mathbb{P}(\{2,4,6)\}) = \mathbb{P}(\{2\} \cup \{4\} \cup \{6\}) = \mathbb{P}(\{2
\}) + \mathbb{P}(\{4\})
+ \mathbb{P}(\{6\}) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}
\end{equation}

Notice that this calculation is rather cumbersome. After all, we might just have evaluated 
$ \mathbb{P}(\{2,4,6)\}) $ directly. However, the above calculation points to an interesting fact. In order
to fully specify a probability measure, is suffices to specify the measure on the singleton sets of the
event space. By countable additivity this specifies the measure on the entire event space as we can
construct any event as a countable union of singletons.

It is important to point out that we just chose the uniform probability measure as the one that seems ``natural'' for
a die roll. However, nobody is forcing us to do so. In fact, def. \ref{ProbabiltySpace} allows us to impose arbitrary
probability measures.

\begin{Exercise}
Take $ (\Omega, \mathcal{A}, \mathbb{P}) $ with $ \Omega $ and $ \mathcal{A} $ as before but now use the 
probability measure \\ $ \mathbb{P} = \{(\{1\},0), (\{2\}, \frac{1}{12}), (\{3\}, \frac{1}{6}), (\{4\}, \frac{1}{6}), (\{5\}, \frac{1}{3}),
(\{6\},\frac{1}{4}) \} $.
\begin{enumerate}
\item Verify that $ \mathbb{P} $ is indeed a probability measure.
\item Compute the probability of obtaining a number smaller than $ 5 $ in this experiment.
\end{enumerate}
\end{Exercise}

\begin{figure}
\center
\begin{subfigure}{.4\textwidth}
\begin{venndiagram2sets}[labelA=$ E_{1} $, labelB= $ E_{2} $, labelAB= $ E_{3} $, shade=red!40]
\fillACapB
\end{venndiagram2sets}
\caption{}
\label{Venn2}
\end{subfigure}
~
\begin{subfigure}{.4\textwidth}
\begin{venndiagram3sets}[labelA=$ E_{1} $, labelB=$ E_{2} $, labelC=$ E_{3} $, labelOnlyAB=$ - $, 
labelOnlyBC=$ - $, labelOnlyAC=$ - $, labelABC=$ + $, shade=red!40]
\fillACapB
\fillACapC
\fillBCapC
\end{venndiagram3sets}
\caption{}
\label{Venn3}
\end{subfigure}
\caption{\ref{Venn2}: Two overlapping events $ E_{1} $ and $ E_{2} $. Their intersection 
(the coloured region) gets counted twice if we add up their probabilities. \\
\ref{Venn3}: Venn diagram with 3 events. First we deduct 
$ E_{1} \cap E_{2}, E_{1} \cap E_{3}, E_{2} \cap E_{3} $ in order to prevent double counting and then
we add in $ E_{1} \cap E_{2} \cap E_{3} $. Deductions and additions are indicated by pluses and minuses.}
\end{figure}

We have now seen how to compute probabilities of events if they can be formed as unions of \textit{disjoint}
events. The natural question to ask is what to do if we want to compute the probability of the union
of non-disjoint events. In order to reason about this, we first take a step back and think about the
outcomes of our probability space. We know that each relevant event contains at least one outcome (since
$ \mathbb{P}(\emptyset) = 0 $ we can safely ignore it). Now let us assume that we take the union of events
$ E_{1} $ and $ E_{2} $ with $ E_{1} \cap E_{2} = E_{3} \not = \emptyset $. This means that the outcomes
in $ E_{3} $ are contained in both $ E_{1} $ and $ E_{2} $. This situation is illustrated in fig. 
\ref{Venn2}. Now if we apply the rule that we have for disjoint events, we get 
\begin{align} \label{InExFalse}
\mathbb{P}(E_{1} \cup E_{2}) =& \mathbb{P}(E_{1}) + \mathbb{P}(E_{2})
= \mathbb{P}(((E_{1}\backslash E_{3}) \cup E_{3}) \cup ((E_{2}\backslash E_{3}) \cup E_{3})) \\
=& \mathbb{P}(E_{1}\backslash E_{3}) + \mathbb{P}(E_{3}) + \mathbb{P}(E_{2}\backslash E_{3}) \nonumber
+ \mathbb{P}(E_{3})
\end{align}

\begin{Exercise}
Try to justify each of the above equalities using your knowledge about sets, event spaces and the 
probability measure. In doing so, you will realize that one of the above equalities does not hold.
Which one? 
\end{Exercise}

What happens here is that we count $ E_{3} $, the outcomes that are in both events of interest, twice.
Thus, what we will need to is to substract $ E_{3} $ one time. This leads us to the following formulation:
\begin{equation}
\mathbb{P}(E_{1} \cup E_{2}) = \mathbb{P}(E_{1}) + \mathbb{P}(E_{2}) - \mathbb{P}(E_{1} \cap E_{2})
\end{equation}

Notice that this is fully general in that it is true even if $ E_{1} $ and $ E_{2} $ were disjoint. In that
case, their intersection would be empty. We can generalize this principle to the (countable) union of
an arbitrary number of events. This will give us a principled way of calculating the probability of any
union of events. This calculation technique is know as the \textbf{Inclusion-Exclusion principle}.

\begin{Theorem}[Inclusion-Exclusion principle]
The probability of any (countable) union of events $ E_{1}, \ldots, E_{n} $ can be computed as \\
$$ \mathbb{P} \left( \underset{i=1}{\overset{n}{\bigcup}} E_{i} \right) 
= \underset{i=1}{\overset{n}{\sum}} (-1)^{i+1} \left( \underset{j_{1}<\ldots<j_{i}}{\sum} 
\mathbb{P} \left(E_{j_{1}} \cap \ldots \cap E_{j_{i}} \right) \right)  $$
\end{Theorem}

We are now going to proceed with a combinatorial proof of the Inclusion-Exclusion principle. It is
very elegant but invokes the \href{https://en.wikipedia.org/wiki/Binomial_theorem}{binomial theorem}
which we do not expect you to know. For completeness sake we will prove the binomial theorem
at the end of this chapter. For now, just trust us that it exists and is correct.

We are going to focus on a particular outcome $ \omega $ that is contained in $ m $ events 
$ E_{1}, \ldots, E_{m} $. Notice that we can safely neglect all events which do not contain
$ \omega $, since $ \omega $ is not going to contribute to their probability. 

For all the $ E_{i} $, $ 1 \leq i \leq m $ in which $ \omega $ is contained, it is certainly true
that $ \omega $ is also contained in their intersections. Now the Inclusion-Exclusion-principle
adds up the probabilities of intersections of a given size. Notice that any intersection of more
than $ m $ events will not contain $ \omega $ as we intersect with at least one event that does not
contain $ \omega $. Thus, we only need to consider intersections of our $ \omega $-containing $ m $
sets.

The first intersection is trivial, as it just consists of one event. How many ways are there to pick
one out of $ m $ events? The answer is $ \binom{m}{1} $. This is the number of times that $ \omega $
contributes to the overall probability. At this point we have an overestimate of that probability 
(compare this to eq. \ref{InExFalse}). Next we subtract the probabilities of the mutual intersections.
By the same reasoning as before, the contribution of $ \omega $ is deducted $ \binom{m}{2} $ times
which gives us an underestimate since $ \binom{m}{1} \leq \binom{m}{2} $ for $ m \geq 3 $.

What we want is that in total we only count $ \omega $'s contribution to the overall probability once.
That is we want that
\begin{equation} \label{InExProofStep1}
1 = \underset{i=1}{\overset{m}{\sum}}(-1)^{i-1}\binom{m}{i}
\end{equation}

And now we are right on our way towards exploiting the binomial theorem. Let us first state it.
\begin{equation} \label{binomTheorem}
(p + q)^{m} = \underset{i=0}{\overset{m}{\sum}} \binom{m}{i} p^{i}q^{n-i} 
\end{equation}

It may not be entirely obvious how to use the theorem in our situation, but hold on. First observe
that the sum in eq. \ref{binomTheorem} actually starts counting at 0 whereas in eq. \ref{InExProofStep1}
it starts counting from 1. However, we already know that $ \binom{m}{0} = 1 $. Thus we can replace the
the left hand side in eq. \ref{InExProofStep1} with this binomial coefficient and then subtract 1 from the
right hand side. This gives us \ref{InExProofStep2}. This is the equality that we are going to proof
in what follows. Since we derived it from eq. \ref{InExProofStep1}, it implies that if \ref{InExProofStep2}
holds, so does \ref{InExProofStep1}.
\begin{equation} \label{InExProofStep2}
\underset{i=0}{\overset{m}{\sum}}(-1)^{i}\binom{m}{i} = 0
\end{equation} 

Now we notice that $ 0 $ in turn can be expressed as $ (1-1) $. Furthermore, any power of 0 is still 0.
Thus we rewrite again to get
\begin{equation}
\underset{i=0}{\overset{m}{\sum}}\binom{m}{i}(-1)^{i} = (1-1)^{m}
\end{equation}

This now looks a lot like the binomial theorem, doesn't it? Just one component is missing. We have
our $ q $ from eq. \ref{binomTheorem} which is $ -1 $ in this case. We still need to account for 
$ p $, though. Luckily, setting $ p=1 $ and taking powers does not change anything. Thus we can safely write
\begin{equation}
\underset{i=0}{\overset{m}{\sum}}\binom{m}{i}(-1)^{i}1^{m-i} = (1-1)^{m}
\end{equation} 

This equality is now true by the binomial theorem (eq. \ref{binomTheorem}) which it is an instantiation of.
We have thus succeeded in proofing the Inclusion-Exclusion principle. If the proof is not entirely
clear to you, please go over it again.

At this point we have done our fair share of math and found out how to calculate the probability of a union
of events. Now we should ask ourselves, what the probability of a union of events even tells us. Observe that an event 
occurs whenever we draw an outcome from our sample space that is contained in that event. By taking the union
of events $ E_{1}, \ldots, E_{n} $ we form a new event $ E $ that (possibly) contains more outcomes than each 
of the original events. Thus, the probability of the $ E $ will be higher than (or the same as) the 
probability of each of $ E_{1}, \ldots, E_{n} $. What we are measuring then, is the probability that 
\textit{any} of the events $ E_{1}, \ldots, E_{n} $. Crucially, we do not care anymore which one of them
occurs.

What we are missing is a way to express the probability that a given number of events occur 
\textit{together}. This concept is so important that we have a dedicated name for it, that of 
\textbf{joint probability}.

\begin{Definition}{}
The joint probability of a (countable) set of events \\ $ \{E_{1}, \ldots, E_{n}\} $ is defined as
$$ \mathbb{P}(E_{1}, \ldots, E_{n}) = \mathbb{P}(E_{1} \cap \ldots \cap E_{n}) $$
\end{Definition}

Wow, that was simple! We don't event need to prove another rule for calculating the joint probability.
After all, we already know how to take the intersection of sets. Annoyingly, one problem remains: our
definition of event spaces does not guarantee that they contain the intersections of their members. Or does 
it? Well, let us see whether we can ``paraphrase'' what an intersection is.

\begin{align}
E_{1} \cap E_{2} = \Omega \backslash ((\Omega \backslash E_{1}) \cup (\Omega \backslash E_{2}))
\end{align}

\textbf{[REMARK PHILIP: Should we maybe put this into chapter one when we introduce event spaces?]}

All the operations on the right hand side are defined for events spaces. We have thus solved our problem.
To convince yourself that this is correct, you may want to consult fig. \ref{Venn2}. We also do not claim
that this is the only valid ``paraphrase''. Feel free to find others, if you like!

At this point we are capable to do most probabilistic computations that we will encounter in this course.
From here on, it is all about making our lives easier. For example, how would you solve the following 
problem.

\begin{Exercise}
You are observing a panel of 200 light bulbs and you know that at least one of them will light up once you 
press a button. What is the probability that any except the 87th bulb will light up? Note: this is a 
conceptual exercise. For the very keen ones, you can obtain the probability for each bulb to be turned on
by typing the following into the Python interpreter:

\begin{lstlisting}
import numpy

probabilities = numpy.random.rand(1,200)
probabilities/probabilities.sum()
\end{lstlisting} 
\end{Exercise}

The point of the above exercise is that it will be awfully cumbersome to compute the probability of the
union of the singletons $ {E_{i}} $ where $ 1 \leq i \leq 200 $ and $ i \not = 87 $. On the other hand
we can easily look up $ \mathbb{P}(E_{87}) $. The question is whether we can exploit this simpler calculation
to help us answer the original question. Here we will again make use of the properties of event spaces.
For any event $ E $ in our event space we also have $ \Omega \backslash E $ in the same space. Furthermore,
$ E $ and $ \Omega \backslash E $ are disjoint which by our probability axioms means that we can simply add
up their probabilities if we want to calculate the probability of their union. But what's the union
of $ E $ and $ \Omega \backslash E $? It's exactly $ \Omega $. From our axioms we know that 
$ \mathbb{P}(\Omega) = 1 $. By simple algebraic manipulations we find that 
\begin{equation}
\mathbb{P}(\Omega \backslash E) = 1 - \mathbb{P}(E)
\end{equation}

Thus if we want to find the probability that any but the $ 87th $ bulb will light up, we simply compute
the probability that it will light up and subtract that from 1. This is a rather general strategy that
comes in handy whenever the probability of an event is hard to compute. Maybe the probability of the
complement of that event will be easier to compute.

\begin{Exercise}
Show that in general $$ \mathbb{P}(E_{1}\backslash (E_{1}\cap E_{2})) 
= \mathbb{P}(E_{1}) - \mathbb{P}(E_{1}\cap E_{2}) $$
\end{Exercise}

\section{Conditional probability and independence}
After we have seen how to measure the probability of events, we are now going to introduce another
tremendously important concept, that of \textbf{conditional probability} measures.

\begin{Definition}{}
The probability of an event $ E_{i} $ conditioned on another event $ E_{j} $ is defined as
$$ \mathbb{P}(E_{i}|E_{j}) = \dfrac{\mathbb{P}(E_{i} \cap E_{j})}{\mathbb{P}(E_{j})} $$
The function $ \mathbb{P}(\cdot|E_{j}) $ is often referred to as conditional probability measure.
\label{condProb}
\end{Definition}

Before we get into the math of conditional probabilities, let us try to understand what this concept event
signifies. When we are computing the conditional probability of an event $ E_{i} $, we scale it to the 
probability of the conditioning event $ E_{j} $. Now if $ E_{j} \not = \Omega $, $ \mathbb{P}(E_{j}) $
might be smaller than 1. Thus, this rescaling assumes \textit{that $ E_{j} $ has already occurred}. In other
words, we are excluding all outcomes that are not in $ E_{j} $ from further consideration (event though they
may be in $ E_{i} $). The interpretation of conditional probabilities that is that they are the probabilities
of events assuming that another event has already occurred.

Another interpretation is that when working with a conditional probability measure, we are in fact working
in a new probability space, where $ \Omega_{new} = E_{2} $, i.e. our sample space is the conditioning event.
Notice that this also means that our probability measure will change. In particular it will become the
measure from def. \ref{condProb}.

Now here comes the cool part: although we have introduced a new concept, all the properties of probability
measures that we know by now will seamlessly carry over to conditional probabilities if we can prove
that the conditional probability measure is a probability measure according to our axioms.

\begin{Exercise}
Use the axioms from def. \ref{axioms} to prove that $ \mathbb{P}(\cdot|E_{j}) $ is a probability measure.
\end{Exercise} 

We will make use of conditional probabilities quite a lot in this course. We will later see a way in which
they help us to decompose joint probability distributions. For now, we are going to focus on the fact that
they are also related to the idea of \textbf{independence} between events.

\begin{Definition}
Two events $ E_{1}, E_{2} $ are said to be independent if 
$$ \mathbb{P}(E_{1}, E_{2}) = \mathbb{P}(E_{1}) \times \mathbb{P}(E_{2}) $$
Independence between events is denoted as $ E_{1} \bot E_{2} $.
\end{Definition}

This definition relates to conditional probabilities in the following way: assume that $ E_{1} \bot E_{2} $.
The we get
\begin{equation}
\mathbb{P}(E_{1}|E_{2}) = \dfrac{\mathbb{P}(E_{1} \cap E_{2})}{\mathbb{P}(E_{2})}
= \dfrac{\mathbb{P}(E_{1}) \times \mathbb{P}(E_{2})}{\mathbb{P}(E_{2})} = \mathbb{P}(E_{1})
\end{equation}

Notice however, that
$ E_{1} \bot E_{2} \Rightarrow \mathbb{P}(E_{1}|E_{2}) = \mathbb{P}(E_{1}) $ but not the other way around.

\begin{Exercise}
Find a probability space with at least three events such that $ \mathbb{P}(E_{1}|E_{2}) = \mathbb{P}(E_{1)} $
but $ \mathbb{P}(E_{1}, E_{2}) \not = \mathbb{P}(E_{1}) \times \mathbb{P}(E_{2}) $.
\end{Exercise}

Independence will prove to be a useful concept in later chapters. More precisely, we will often
just \textit{assume} that two events (or random variable -- see the next chapter) are independent. Although
this assumption may not be true, it will allow us to formulate much simpler probabilistic models.


\section{A Remark on the interpretation \\ of probabilities$^{*}$}

This concludes our introduction of axiomatic probability theory. We now know that a probability is
a real number in $ [0,1] $. For all that we are going to do in this course (and in most follow-up courses)
this is fully sufficient. However, some of you may wonder what a ``natural'' interpretation of probabilities
would be. There are two dominating views on that. One postulates that we were to take A LOT (read: almost
infinitely many) samples from a sample space, the probability of an event is its frequency amongst these
samples divided by the total number of samples taken. For those of you who know limits, this can be
formalized as $ \mathbb{P}(E) = \underset{n \rightarrow \infty}{lim} \dfrac{\#E}{n} $. This view
is known as the frequentist view.

The second view postulates that probabilities are an expression for degrees of belief. Basically, 
if you assign $ \mathbb{P}(E) $ to an event $ E $, then $ \mathbb{P}(E) $ is the strength your personal belief that
$ E $ will occur. This latter view is know as the Bayesian view.

Which conception of probability you choose is a philosophical matter and does not really impact the math.
That is why we will not care about this issue in this course. However, it is useful to at least be aware
of these two views (if only to appear knowledgeable in a conversation you may have with your philosopher 
friends).


\section{The binomial theorem$^{*}$}
The binomial theorem from eq. \ref{binomTheorem} is actually not that hard to prove. We will do so by
induction. As a base case we choose $ m = 0 $. Then the equality is easy to see.
\begin{equation}
(p + q)^{0} = 1 = \binom{0}{0}p^{0}q^{0}
\end{equation}

Next, we assume that the theorem holds for $ m = n $. What we want to show is that it also holds for
$ m = n + 1 $. We achieve this by algebraic manipulation.

\begin{align}
(p + q)^{n+1} &= (p + q)^{n} \times (p + q) \\
&= (p+q)^{n}p + (p+q)^{n}q \\
&= p\underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n-i} + q\underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n-i} \\
&= \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i+1}q^{n-i} + \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n+1-i} \\
&= \underset{j=1}{\overset{n+1}{\sum}} \binom{n}{j-1} p^{j}q^{n+1-j} + \underset{i=0}{\overset{n}{\sum}} \binom{n}{i} p^{i}q^{n+1-i} \label{variableSwitch} \\
&= \binom{n}{n} p^{n+1}q^{(n+1)-(n+1)} + \underset{k=1}{\overset{n}{\sum}} \binom{n}{k-1} p^{k}q^{n+1-k} \label{pullOut} \\
&+ \binom{n}{0} p^{0}q^{n+1} + \underset{k=1}{\overset{n}{\sum}} \binom{n}{k} p^{k}q^{n+1-k} \nonumber \label{collapseSums} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\binom{n}{k} + \binom{n}{k-1}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!}{k!(n-k)!} + \dfrac{n!}{(k-1)!(n-k+1)!}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!(n+1-k)}{k!(n+1-k)!} + \dfrac{n!k}{k!(n-k+1)!}\right) p^{k}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \left(\dfrac{n!(n+1)}{k!(n+1-k)!}\right) p^{i}q^{n+1-k} \\
&= q^{n+1} + p^{n+1} + \underset{k=1}{\overset{n}{\sum}} \binom{n+1}{k} p^{k}q^{n+1-k} \\
&= \underset{i=0}{\overset{n+1}{\sum}} \binom{n}{k} p^{k}q^{n-k}
\end{align}

Let us clarify some parts of the proof. In line \ref{variableSwitch}, we switch the variable $ i $ in the first summand to $ j = i+1 $. The
reason why we do this is because we want to achieve congruence with the exponents of the second summand. In the following line we 
uniformly name the variables $ k $. Since $ k $ has to run over a common range, we chop off the ends of both sums that stick out. In the first
sum of line \ref{variableSwitch} that is the summand that corresponds to $ j=n+1 $ and in the second sum it is the summand that corresponds
to $ i = 0 $. We pull out both of them in line \ref{pullOut} and then collapse the sums in line \ref{collapseSums}. The following lines 
are basically just an exercise in manipulation fractions. The jump from the second-to-last to the last line is allowed because
$$ q^{n+1} = \binom{n+1}{0}p^{0}q^{n+1-0} $$ and $$ p^{n+1} = \binom{n+1}{n+1}p^{n+1}q^{(n+1)-(n+1)} $$
which are exactly the quantities that we need to add to make our sum reach from $ 0 $ to $ n+1 $. This completes the proof.

\section*{Further reading}
A very quick and dirty introduction to measure theory is provided by Maya Gupta and can be found 
\href{https://www.ee.washington.edu/techsite/papers/documents/UWEETR-2006-0008.pdf}{here}. If you are
looking for something more extensive that also motivates event spaces and the like you may want to 
take a look at \href{http://www.stat.ncsu.edu/people/fuentes/courses/st778/lectures/ross}{this script}
by Ross Leadbatter and Stamatis Cambanis (which has also been published as a book).

\end{document}